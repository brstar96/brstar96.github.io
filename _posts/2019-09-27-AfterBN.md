---
title: "(Normalization) BN이후의 다양한 정규화 기법들"
tags: 
  - Deep Learning
  - Normalization
categories:
  - MLDLstudy
toc: true
comments: 
  provider: "disqus"
  disqus:
    shortname: "https-brstar96-github-io"
use_math: true
header:
  teaser: /assets/Images/Norm-pic.png
---

<Blockquote><span style="font-size:11pt">본 글은 개인적으로 스터디하며 정리한 자료입니다. 간혹 레퍼런스를 찾지 못해 빈 곳이 있으므로 양해 부탁드립니다.</span></Blockquote>

<center><img src="/assets/Images/Norm-pic.png"></center>

### [Weight Normalization(WN)](https://arxiv.org/pdf/1602.07868.pdf)

- <span style="font-size:11pt">Weight normalization은 mini-batch를 정규화하는 것이 아니라 layer의 가중치를 정규화</span>
- <span style="font-size:11pt">Weight normalization은 레이어의 가중치 w를 다음과 같이 재매개변수화(reparametrization)한다.</span><br> 
  $w=\frac{g}{\|v\|}v$
- <span style="font-size:11pt">BN과 비슷하게 WN은 표현(expressiveness)을 줄이지 않고 가중치 벡터의 크기와 방향을 분리한다. (BN에서 입력값을 표준편차로 나누어 주는 것과 비슷한 효과)<span>
- <span style="font-size:11pt">이후 Gradient descent기법(경사하강법)으로 g, v를 최적화하며, 이는 학습의 최적화를 쉽게 만듬. WN은 경우에 따라 BN보다 빠름.</span> 
- <span style="font-size:11pt">CNN의 경우 가중치의 수는 입력의 수보다 훨씬 작으며, 이는 즉 BN보다 WN이 연산량이 훨씬 적음을 의미. (BN의 경우 입력값의 모든 원소를 연산해야 하고, 이미지 등의 고차원 데이터에서 연산이 매우 많아진다.)</span>
- <span style="font-size:11pt">WN은 그 자체만으로도 모델 훈련에 도움을 주지만 mean-only batch normalization(입력을 표준편차로 나누거나 scale 재조정을 하지 않는 BN)과 함께 사용하는 것을 권장</span>
    - <span style="font-size:11pt">표준편차를 계산하지 않기 때문에 BN보다 연산량이 적음.</span> 
    - <span style="font-size:11pt">활성값의 평균을 v와 독립시킬 수 있다. (WN은 활성화값의 평균과 레이어의 가중치를 독립적으로 분리할 수 없으므로 각 레이어의 평균간에 높은 종속성이 발생. mean-only batch normalization를 사용하면 이런 문제를 해결할 수 있다고 주장)</span>
	- <span style="font-size:11pt">활성화에 gentler noise를 추가 (BN의 부작용 중 하나는 mini-batch에서 계산된 노이즈가 많은 추정값을 사용해 활성화값에 확률적인 잡음을 추가한다는 것이며, 일부 문제에서 이런 노이즈는 규제의 역할을 수행하지만 강화 학습과 같이 노이즈에 예민한 태스크에서는 성능을 낮춘다.)</span>
        - <span style="font-size:11pt">이 경우 mean-only batch normalization과 WN을 함께 사용하면 큰 수의 법칙에 따라 노이즈가 정규 분포의 형태를 띄며, 이는 노이즈가 완만하다고 할 수 있다.</span> 
		- <span style="font-size:11pt">BN과 비교해 훈련 과정에서 노이즈가 줄어들게 된다.</span> 
		- <span style="font-size:11pt">CIFAR-10모델에서 이미지 분류 문제를 해결할 때 WN이 특히 잘 작동한다고 함.</span> 

### [Layer Normalization(LN)](https://arxiv.org/pdf/1607.06450.pdf)
BN과 유사한 형태를 지니며, mini-batch의 feature 수가 같아야 한다. <br>

<center><img src="/assets/Images/LN.png"></center>

- <span style="font-size:11pt">BN과 LN의 차이점</span>
    - <span style="color:red">BN은 batch 차원에서의 정규화, LN은 feature 차원에서의 정규화임에 유의</span> 